}
crs(spobj1) <- "+proj=longlat +datum=WGS84 +no_defs +ellps=WGS84 +towgs84=0,0,0"
crs(spobj2) <- "+proj=longlat +datum=WGS84 +no_defs +ellps=WGS84 +towgs84=0,0,0"
over <- names(which( ! is.na(over(spobj1,spobj2) )))
return(over)
}
## -----------------------------------------------------------------------------------------------
prepareModelData <- function(p,a,env) {
m <- subset(env,1)
p.i <- extract(m,p)
p <- p[which(!is.na(p.i)),]
a.i <- extract(m,a)
a <- p[which(!is.na(a.i)),]
return(prepareSWD(species = "Model species", p = p, a = a, env = env))
}
## -----------------------------------------------------------------------------------------------
removeNA <- function(records,lonName,latName) {
cat("Removing",sum( is.na( records[,lonName] ) ),"NA records")
records <- records[which( ! is.na(records[,lonName]) ), ]
records <- records[which( ! is.na(records[,latName]) ), ]
return(records)
}
## -----------------------------------------------------------------------------------------------
removeDuplicated <- function(records,lonName,latName) {
cat("Removing",length(which( duplicated( records[,c(lonName,latName)] ) )),"duplicated records")
records <- records[ which( ! duplicated( records[,c(lonName,latName)] ) ), ]
return(records)
}
## -----------------------------------------------------------------------------------------------
trainGLM <- function(modelData) {
data <- modelData@data
data <- data.frame(PA=modelData@pa,modelData@data)
model <- glm( paste0("PA ~ ",paste(colnames(modelData@data),collapse = " + ")) , family="binomial", data=data)
return(model)
}
## -----------------------------------------------------------------------------------------------
pseudoAbsences <- function(rasters,records,n) {
shape <- subset(rasters,1)
nonNACells <- Which(!is.na(shape), cells=TRUE)
sink.points <- xyFromCell(shape, nonNACells)
absences <- sample( 1:nrow(sink.points) , min(n,nrow(sink.points)) , replace=FALSE)
absences <- sink.points[absences,]
colnames(absences) <- c("Lon","Lat")
# Removes those closer than paDist
sink.points.poly <- as.data.frame(records)
coordinates( sink.points.poly ) <- c( "Lon", "Lat" )
proj4string( sink.points.poly ) <- CRS( "+proj=longlat +datum=WGS84" )
sink.points.poly <- gBuffer( sink.points.poly, width=50 / 111.699, byid=TRUE )
# plot(sink.points.poly)
sink.points.pts <- as.data.frame(absences)
colnames( sink.points.pts ) <- c( "Lon", "Lat" )
coordinates( sink.points.pts ) <- c( "Lon", "Lat" )
proj4string( sink.points.pts ) <- CRS( "+proj=longlat +datum=WGS84" )
to.remove.id <- sp::over(sink.points.pts,sink.points.poly)
to.keep <- which(is.na(to.remove.id))
absences <- absences[to.keep,]
absences <- as.data.frame(absences)
return(absences)
}
## -----------------------------------------------------------------------------------------------
backgroundInformation <- function(rasters,n) {
shape <- subset(rasters,1)
nonNACells <- Which(!is.na(shape), cells=TRUE)
sink.points <- xyFromCell(shape, nonNACells)
absences <- sample( 1:nrow(sink.points) , min(n,nrow(sink.points)) , replace=FALSE)
absences <- sink.points[absences,]
colnames(absences) <- c("Lon","Lat")
return(absences)
}
## -----------------------------------------------------------------------------------------------
trainBRT <- function(data, distribution = "bernoulli", n.trees = 1000,
interaction.depth = 1, shrinkage = 0.1,
bag.fraction = 0.5 ) {
result <- SDMmodel(data = data)
df <- data@data
df <- cbind(pa = data@pa, df)
if( exists("monotonicity") ) {
model <- gbm::gbm(pa ~ ., data = df, distribution = distribution,
n.trees = n.trees, interaction.depth = interaction.depth,
shrinkage = shrinkage, bag.fraction = bag.fraction, var.monotone=monotonicity[,which(colnames(monotonicity) %in% colnames(df))])
}
if( ! exists("monotonicity")) {
model <- gbm::gbm(pa ~ ., data = df, distribution = distribution,
n.trees = n.trees, interaction.depth = interaction.depth,
shrinkage = shrinkage, bag.fraction = bag.fraction)
}
model_object <- BRT(n.trees = n.trees, distribution = distribution,
interaction.depth = interaction.depth,
shrinkage = shrinkage, bag.fraction = bag.fraction,
model = model)
result@model <- model_object
return(result)
}
environment(trainBRT) <- asNamespace('SDMtune')
assignInNamespace("trainBRT", trainBRT, ns = "SDMtune")
## -----------------------------------------------------------------------------------------------
getAUC <- function(model, test = NULL) {
return(SDMtune::auc(model, test = test))
}
## -----------------------------------------------------------------------------------------------
thresholdMaxTSS <- function(model) {
r <- getAccuracy(model,threshold = seq(0,1,by=0.01))
plot(r$threshold,r$sensitivity,type="l", xlab="Threshold",ylab="Performance" , lty=1, lwd=1 )
lines(r$threshold,r$specificity , lty=1, lwd=4 , col= "gray")
val <- unlist(r$threshold)[which.max( unlist(r$sensitivity) + unlist(r$specificity) )]
abline(v=val , lty=3, lwd=0.7 )
legend(0.7, 0.9, legend=c("Sensitivity", "Specificity"),col=c("black", "gray"), lty=1, lwd=c(1,4) , cex=1)
return(val)
}
## -----------------------------------------------------------------------------------------------
getAccuracy <- function(model,threshold=0.5) {
predMain <- predict(model, model@data@data, type=c("logistic"))
acc <- data.frame()
for(threshold.i in threshold ) {
pred <- predMain
pred[pred >= threshold.i] <- 1
pred[pred < threshold.i] <- 0
confusionDF <- data.frame(obs=model@data@pa,pred)
sensitivity <- sum(apply(confusionDF[confusionDF$obs == 1,],1,function(x) { ifelse(x[1] == 1 & x[2] == 1 , 1 , 0 ) })) / sum(confusionDF$obs == 1)
specificity <- sum(apply(confusionDF[confusionDF$obs == 0,],1,function(x) { ifelse(x[1] == 0 & x[2] == 0 , 1 , 0 ) })) / sum(confusionDF$obs == 0)
acc <- rbind(acc,data.frame(threshold=threshold.i,tss=specificity+sensitivity-1,sensitivity=sensitivity,specificity=specificity))
}
return(acc)
}
# Load main functions
source("sourceFunctions.R")
# Read polygon defining global landmasses
world <- ne_countries(scale = 'medium')
# download the records from GBIF
recordsGBIF <- getOccurrencesGBIF("Laminaria ochroleuca")
# download the records from Obis
recordsObis <- getOccurrencesObis("Laminaria ochroleuca")
# open additional datasets with read.csv
recordsExternalFile <- read.csv("Data/dataBases/gbif.csv", sep=";")
colnames(recordsGBIF)
colnames(recordsObis)
colnames(recordsExternalFile)
# subset objects to get coordinates only
recordsGBIF <- recordsGBIF[,c("Lon","Lat")]
recordsObis <- recordsObis[,c("Lon","Lat")]
recordsExternalFile <- recordsExternalFile[,c("decimalLongitude","decimalLatitude")]
# check for the names of the colnames
# If needed, change column names to allow rbind() function
colnames(recordsGBIF)
colnames(recordsObis)
colnames(recordsExternalFile)
colnames(recordsExternalFile) <- c("Lon","Lat")
# merge datasets
records <- rbind(recordsGBIF,recordsObis,recordsExternalFile)
# Plot the biodiversity records.
plot(world, col="Gray", border="Gray", axes=TRUE, main="Distribution records" , ylab="latitude", xlab="longitude")
points(records[,c("Lon","Lat")], pch=20, col="Black")
records
removeNA(records,"Lon","Lat")
rownames(records)
# remove NA coordinates
records <- removeNA(records,"Lon","Lat")
rownames(records)
# remove duplicate coordinates
records <- removeDuplicated(records,"Lon","Lat")
rownames(records)
# remove NA coordinates
records <- removeNA(records,"Lon","Lat")
# remove duplicate coordinates
records <- removeDuplicated(records,"Lon","Lat")
## based on a distance (km) to shore [alternative]
records <- removeOverLandDist(records, "Lon", "Lat", dist = 9)
rownames(records)
# produce and plot a polygon defining the region of interest
world <- ne_countries(scale = 'medium')
myExtent <- c(-35,30,-10,70.5)
myRegion <- crop(world,extent(myExtent))
plot(myRegion,col="gray",border="gray")
points(records,col="red")
# choose the region where the species occur
regionOfInterest <- drawPoly()
head(records)
# clip the records of occurrence
pointsInRegion <- whichOverPolygon(records, regionOfInterest)
# clip the records of occurrence
records <- records[pointsInRegion, ]
plot(myRegion,col="gray",border="gray")
points(records,col="red")
# remove records outside the known vertical distribution (example at 80m depth)
bathymetry <- raster("Data/rasterLayers/BathymetryDepthMean.tif")
plot(bathymetry)
depthUse <- extract(bathymetry,records[,c("Lon","Lat")])
head(depthUse)
hist(depthUse,breaks=50)
records <- records[ which(depthUse > -80) ,]
# plot records with plot function
plot(myRegion, col="Gray", border="Gray", axes=TRUE, main="Clean distribution records" , ylab="latitude", xlab="longitude")
points(records[,c("Lon","Lat")], pch=20, col="Black")
# plot records with ggplot
ggplot() +
geom_polygon(data = myRegion, fill = "#B9B8B0", colour = "#707070", size = 0.2, aes(x = long, y = lat, group = group)) +
geom_point(data = records, aes(x = Lon, y = Lat), color = "#9A3B04") +
scale_y_continuous(breaks = seq(-90,90, by=20)) +
scale_x_continuous(breaks = seq(-180,180,by=20)) +
coord_fixed() +
xlab("Longitude") + ylab("Latitude") + ggtitle("Clean distribution records")
source("sourceFunctions.R")
# save data frame to external file
write.table(records,file="Data/myFile.csv",sep=";")
# load clean occurrence data with two columns only for Lon and Lat (follow Recipe 1)
presences <- read.csv("Data/myFile.csv", sep = ";")
presences
# Set the working directory to where the data is located
setwd()
# Load main functions
source("sourceFunctions.R")
# download the records from GBIF
recordsGBIF <- getOccurrencesGBIF("Laminaria ochroleuca")
# download the records from Obis
recordsObis <- getOccurrencesObis("Laminaria ochroleuca")
# open additional datasets with read.csv
recordsExternalFile <- read.csv("Data/dataBases/gbif.csv", sep=";")
# check for the names of the colnames
colnames(recordsGBIF)
colnames(recordsObis)
colnames(recordsExternalFile)
# subset objects to get coordinates only
recordsGBIF <- recordsGBIF[,c("Lon","Lat")]
recordsObis <- recordsObis[,c("Lon","Lat")]
recordsExternalFile <- recordsExternalFile[,c("decimalLongitude","decimalLatitude")]
# check for the names of the colnames
# If needed, change column names to allow rbind() function
colnames(recordsGBIF)
colnames(recordsObis)
colnames(recordsExternalFile)
colnames(recordsExternalFile) <- c("Lon","Lat")
# merge datasets
records <- rbind(recordsGBIF,recordsObis,recordsExternalFile)
# Get the global landmasses
world <- ne_countries(scale = 'medium')
# Plot the biodiversity records.
plot(world, col="Gray", border="Gray", axes=TRUE, main="Distribution records" , ylab="latitude", xlab="longitude")
points(records[,c("Lon","Lat")], pch=20, col="Black")
# remove NA coordinates
records <- removeNA(records,"Lon","Lat")
# remove duplicate coordinates
records <- removeDuplicated(records,"Lon","Lat")
## based on a distance (km) to shore [alternative]
records <- removeOverLandDist(records, "Lon", "Lat", dist = 9)
# produce and plot a polygon defining the region of interest
world <- ne_countries(scale = 'medium')
myExtent <- c(-35,30,-10,70.5)
myRegion <- crop(world,extent(myExtent))
plot(myRegion,col="gray",border="gray")
points(records,col="red")
# choose the region where the species occur
regionOfInterest <- drawPoly()
# clip the records of occurrence
pointsInRegion <- whichOverPolygon(records, regionOfInterest)
# clip the records of occurrence
records <- records[pointsInRegion, ]
plot(myRegion,col="gray",border="gray")
points(records,col="red")
# remove records outside the known vertical distribution (example at 80m depth)
bathymetry <- raster("Data/rasterLayers/BathymetryDepthMean.tif")
plot(bathymetry)
depthUse <- extract(bathymetry,records[,c("Lon","Lat")])
head(depthUse)
hist(depthUse,breaks=50)
records <- records[ which(depthUse > -80) ,]
# plot records with plot function
plot(myRegion, col="Gray", border="Gray", axes=TRUE, main="Clean distribution records" , ylab="latitude", xlab="longitude")
points(records[,c("Lon","Lat")], pch=20, col="Black")
# plot records with ggplot
ggplot() +
geom_polygon(data = myRegion, fill = "#B9B8B0", colour = "#707070", size = 0.2, aes(x = long, y = lat, group = group)) +
geom_point(data = records, aes(x = Lon, y = Lat), color = "#9A3B04") +
scale_y_continuous(breaks = seq(-90,90, by=20)) +
scale_x_continuous(breaks = seq(-180,180,by=20)) +
coord_fixed() +
xlab("Longitude") + ylab("Latitude") + ggtitle("Clean distribution records")
# save data frame to external file
write.table(records,file="Data/myFile.csv",sep=";")
# load clean occurrence data with two columns only for Lon and Lat (follow Recipe 1)
presences <- read.csv("Data/myFile.csv", sep = ";")
plot(presences)
# load layers
layerCodes <- list_layers(datasets = "Bio-ORACLE")
environmentalConditions <- load_layers(c("BO2_tempmin_bdmean", "BO2_tempmax_bdmean", "BO2_dissoxmean_bdmean", "BO2_ppmean_bdmean"))
# crop layers to the European extent
europeanExtent <- extent(-20, 40, 20, 55)
environmentalConditions <- crop(environmentalConditions,europeanExtent)
# plot predictors
plot(environmentalConditions)
# generate pseudo absences
pseudoAbs <- pseudoAbsences(environmentalConditions,presences,n=1000)
myExtent <- c(-35,30,-10,70.5)
myRegion <- crop(world,extent(myExtent))
plot(myRegion,col="gray",border="gray")
points(presences,col="red")
points(pseudoAbs,col="blue")
# crop layers to the European extent
myExtent <- c(-35,30,-10,70.5)
# Select a set of predictors
environmentalConditions <- load_layers(c("BO2_tempmin_bdmean", "BO2_tempmax_bdmean", "BO2_dissoxmean_bdmean", "BO2_ppmean_bdmean"))
# crop layers to the European extent
myExtent <- c(-35,30,-10,70.5)
europeanExtent <- extent(myExtent)
environmentalConditions <- crop(environmentalConditions,europeanExtent)
# plot predictors
plot(environmentalConditions)
# generate pseudo-absences
pseudoAbs <- pseudoAbsences(environmentalConditions,presences,n=1000)
# Plot pseudo-absences and presences
myExtent <- c(-35,30,-10,70.5)
myRegion <- crop(world,extent(myExtent))
plot(myRegion,col="gray",border="gray")
points(presences,col="red")
points(pseudoAbs,col="blue")
prepareModelData
p=presences
a=pseudoAbs
env=environmentalConditions
m <- subset(env,1)
plot(m)
p.i <- extract(m,p)
p.i
which(!is.na(p.i))
p <- p[which(!is.na(p.i)),]
a.i <- extract(m,a)
a <- p[which(!is.na(a.i)),]
# extract environmental values and make a data.frame with PA information
modelData <- prepareModelData(presences,pseudoAbs,environmentalConditions)
modelData
# Generate cross validation folds
folds <- get.block(presences, pseudoAbs)
# define monotonicity constrains (-1 for negative, +1 for positive, 0 for non-monotonicity)
monotonicity = data.frame(BO2_tempmin_bdmean=+1,BO2_tempmax_bdmean=-1,BO2_dissoxmean_bdmean=+1,BO2_ppmean_bdmean=+1)
monotonicity
# fit a BRT model with cross-validation and
model <- train("BRT", modelData, folds = folds)
# given a set of possible hyperparameter values for BRT
h <- list(interaction.depth = c(1,2,3,4) , shrinkage = c(0.1,0.01,0.001) )
# test all possible combinations of hyperparameter values
exp1 <- gridSearch(model, hypers = h, metric = "auc")
plot(exp1)
exp1@results
exp1@results[which.max(exp1@results$test_AUC),]
# fit a BRT model to the dataset with the best hyperparameter values
model <- train("BRT", modelData, folds = folds , interaction.depth=1, shrinkage=0.1 )
getAUC(model, test = TRUE)
plot(presences)
# crop for maximum potential distribution
bathymetry <- raster("Data/rasterLayers/BathymetryDepthMean.tif")
bathymetry
bathymetry[bathymetry < -80 ] <- NA
bathymetry <- mask(bathymetry,environmentalConditions)
bathymetry <- crop(bathymetry,environmentalConditions)
bathymetry <- mask(bathymetry,environmentalConditions)
plot(bathymetry)
# crop for maximum potential distribution
bathymetry <- raster("Data/rasterLayers/BathymetryDepthMean.tif")
bathymetry <- crop(bathymetry,environmentalConditions)
bathymetry <- mask(bathymetry,environmentalConditions)
# crop for maximum potential distribution
bathymetry <- raster("Data/rasterLayers/BathymetryDepthMean.tif")
bathymetry <- crop(bathymetry,subset(environmentalConditions,1))
bathymetry <- mask(bathymetry,subset(environmentalConditions,1))
plot(bathymetry)
bathymetry[bathymetry < -80 ] <- NA
plot(bathymetry)
environmentalConditions <- mask(environmentalConditions,bathymetry)
# plot predictors
plot(environmentalConditions)
# generate pseudo-absences
pseudoAbs <- pseudoAbsences(environmentalConditions,presences,n=1000)
# Plot pseudo-absences and presences
myExtent <- c(-35,30,-10,70.5)
myRegion <- crop(world,extent(myExtent))
plot(myRegion,col="gray",border="gray")
points(presences,col="red")
points(pseudoAbs,col="blue")
# extract environmental values and make a data.frame with PA information
modelData <- prepareModelData(presences,pseudoAbs,environmentalConditions)
# Generate cross validation folds
folds <- get.block(presences, pseudoAbs)
# define monotonicity constrains (-1 for negative, +1 for positive, 0 for non-monotonicity)
monotonicity = data.frame(BO2_tempmin_bdmean=+1,BO2_tempmax_bdmean=-1,BO2_dissoxmean_bdmean=+1,BO2_ppmean_bdmean=+1)
monotonicity
# fit a BRT model with cross-validation and
model <- train("BRT", modelData, folds = folds)
# given a set of possible hyperparameter values for BRT
h <- list(interaction.depth = c(1,2,3,4) , shrinkage = c(0.1,0.01,0.001) )
# test all possible combinations of hyperparameter values
exp1 <- gridSearch(model, hypers = h, metric = "auc")
plot(exp1)
exp1@results
exp1@results[which.max(exp1@results$test_AUC),]
# fit a BRT model to the dataset with the best hyperparameter values
model <- train("BRT", modelData, folds = folds , interaction.depth=1, shrinkage=0.1 )
getAUC(model, test = TRUE)
prepareModelData
modelData
presences
plot(presences)
p
m <- subset(env,1)
?extract
cellFromXY(m,p)
xyFromCell(m,cellFromXY(m,p))
p <- xyFromCell(m,cellFromXY(m,p))
p <- unique(p)
p
plot(p)
prepareModelData <- function(p,a,env) {
m <- subset(env,1)
p.i <- extract(m,p)
p <- p[which(!is.na(p.i)),]
p <- xyFromCell(m,cellFromXY(m,p))
p <- unique(p)
a.i <- extract(m,a)
a <- p[which(!is.na(a.i)),]
a <- xyFromCell(m,cellFromXY(m,a))
a <- unique(a)
return(prepareSWD(species = "Model species", p = p, a = a, env = env))
}
# extract environmental values and make a data.frame with PA information
modelData <- prepareModelData(presences,pseudoAbs,environmentalConditions)
prepareModelData <- function(p,a,env) {
m <- subset(env,1)
p.i <- extract(m,p)
p <- p[which(!is.na(p.i)),]
p <- xyFromCell(m,cellFromXY(m,p))
p <- unique(p)
a.i <- extract(m,a)
a <- a[which(!is.na(a.i)),]
a <- xyFromCell(m,cellFromXY(m,a))
a <- unique(a)
return(prepareSWD(species = "Model species", p = p, a = a, env = env))
}
# extract environmental values and make a data.frame with PA information
modelData <- prepareModelData(presences,pseudoAbs,environmentalConditions)
# Generate cross validation folds
folds <- get.block(presences, pseudoAbs)
# define monotonicity constrains (-1 for negative, +1 for positive, 0 for non-monotonicity)
monotonicity = data.frame(BO2_tempmin_bdmean=+1,BO2_tempmax_bdmean=-1,BO2_dissoxmean_bdmean=+1,BO2_ppmean_bdmean=+1)
monotonicity
# fit a BRT model with cross-validation and
model <- train("BRT", modelData, folds = folds)
modelData
folds
modelData
names(modelData)
modelData
modelData@coords
modelData@pa
modelData@coords[modelData@pa == 1,]
# Generate cross validation folds
folds <- get.block(modelData@coords[modelData@pa == 1,], modelData@coords[modelData@pa == 0,])
# define monotonicity constrains (-1 for negative, +1 for positive, 0 for non-monotonicity)
monotonicity = data.frame(BO2_tempmin_bdmean=+1,BO2_tempmax_bdmean=-1,BO2_dissoxmean_bdmean=+1,BO2_ppmean_bdmean=+1)
monotonicity
# fit a BRT model with cross-validation and
model <- train("BRT", modelData, folds = folds)
# given a set of possible hyperparameter values for BRT
h <- list(interaction.depth = c(1,2,3,4) , shrinkage = c(0.1,0.01,0.001) )
# test all possible combinations of hyperparameter values
exp1 <- gridSearch(model, hypers = h, metric = "auc")
getBlocks <- function(modelData) {
get.block(modelData@coords[modelData@pa == 1,], modelData@coords[modelData@pa == 0,])
}
# Generate cross validation folds
folds <- getBlocks(modelData)
folds
# define monotonicity constrains (-1 for negative, +1 for positive, 0 for non-monotonicity)
monotonicity = data.frame(BO2_tempmin_bdmean=+1,BO2_tempmax_bdmean=-1,BO2_dissoxmean_bdmean=+1,BO2_ppmean_bdmean=+1)
monotonicity
# test all possible combinations of hyperparameter values
exp1 <- gridSearch(model, hypers = h, metric = "auc")
plot(exp1)
exp1@results
exp1@results[which.max(exp1@results$test_AUC),]
# fit a BRT model to the dataset with the best hyperparameter values
model <- train("BRT", modelData, folds = folds , interaction.depth=2, shrinkage=0.001 )
getAUC(model, test = TRUE)
# determine relative variable contribution
viModel <- varImp(model, permut = 5)
viModel
# reduce model complexity by dropping one variable at a time
reducedModel <- reduceVar(model, th = 5, metric = "auc", permut = 5)
# determine the final performance as AUC
getAUC(reducedModel, test = TRUE)
# determine the final relative variable contribution
viModel <- varImp(reducedModel, permut = 5)
viModel
plotVarImp(viModel)
# inspect response curves
plotResponse(reducedModel, var = "BO2_tempmax_bdmean", type = "logistic", only_presence = FALSE, marginal = FALSE, rug = FALSE, color="Black")
# inspect response curves
plotResponse(reducedModel, var = "BO2_tempmin_bdmean", type = "logistic", only_presence = FALSE, marginal = FALSE, rug = FALSE, color="Black")
# predict with BRT to raster stack
mapPresent <- predict(reducedModel, environmentalConditions, type=c("logistic"))
plot(mapPresent)
# determine the threshold maximizing the sum of sensitivity and specificity
threshold <- thresholdMaxTSS(reducedModel)
threshold
# generate a reclassification table
thresholdConditions <- data.frame(from = c(0,threshold) , to=c(threshold,1) , reclassValue=c(0,1))
thresholdConditions
# apply threshold to reclassify the predictive surface
mapPresentReclass <- reclassify(mapPresent, rcl = thresholdConditions)
plot(mapPresentReclass)
# determine model performance
getAccuracy(reducedModel,threshold = threshold)
getAUC(reducedModel, test = TRUE)
# load layers of sea surface temperatures for the future
environmentalConditionsRCP26 <- load_layers(c("BO2_RCP26_2100_tempmin_bdmean","BO2_RCP26_2100_tempmax_bdmean"))
